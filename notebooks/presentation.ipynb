{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afdaee62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using Azure Machine Learning Studio with Python\n",
    "\n",
    "We will go through some relevant components of AzureML, and how to manage them from the python sdk. The goal is to be able to utilize Azureml for flexible model training and management and build useful and flexible pipeline components.\n",
    "\n",
    "We will discuss:\n",
    "- SDK\n",
    "- compute\n",
    "- environments\n",
    "- models\n",
    "- jobs\n",
    "- pipeline componenents\n",
    "- pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56bc304",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dependencies\n",
    "\n",
    "- Azure CLI\n",
    "```python\n",
    "#libraries for azure machine learning\n",
    "azureml-core\n",
    "mlflow\n",
    "azure-ai-ml\n",
    "\n",
    "#libraries for azure identiy and access management\n",
    "azure-identity\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6607da8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Connect from the SDK\n",
    "\n",
    "All interactions with AzureML through the SDK go through the `MLClient` object, which authenticates either with a `DefaultAzureCredential` or `AzureCLICredential` obtained from your logged in session of azure cli, or by using an `InteractiveBroserCredential`. \n",
    "\n",
    "To create the `MLCLient`, download the config.json file from azureml and store it in the root of your project: \n",
    "\n",
    "<img src=\"./images/configjson.png\" alt=\"Config json download location in Azure ML Studio\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ad4d70",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /home/daniel/repos/aml_demo/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLClient created successfully.\n"
     ]
    }
   ],
   "source": [
    "#create the MLClient object\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient.from_config(credential=credential)\n",
    "\n",
    "print(\"MLClient created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce5f24",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compute\n",
    "We need to provision compute to do our work for us. We can create:\n",
    "- compute instances which are aimed at running notebooks and development\n",
    "- compute clusters which are for our heavy workloads\n",
    "\n",
    "We can also run jobs and commands using `serverless` compute these days. But lets create a cluster. You can do this in the studio manually, or use the sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae372d59",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigcompute\n",
      "defaultcompute\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "compute_name = \"defaultcompute\"\n",
    "helloworld_compute= AmlCompute(name=compute_name, \n",
    "                               size=\"STANDARD_DS3_v2\", \n",
    "                               min_instances=0, \n",
    "                               max_instances=1, \n",
    "                               idle_time_before_scale_down=300)\n",
    "\n",
    "## We already have this in the workspace, so this is just to show how to create a new one. No need to run it.\n",
    "# ml_client.compute.begin_create_or_update(helloworld_compute)\n",
    "\n",
    "for c in ml_client.compute.list():\n",
    "    print(c.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedce56b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Environments\n",
    "\n",
    "Environments are basically docker images stored in the container registry associated with your azureml studio instance. These docker images are used to run your code on the compute resources. You can create environments quite simply from a conda specification, for example:\n",
    "\n",
    "```python\n",
    "%%writefile ../environments/helloworld.yaml\n",
    "name: catsanddogsenv\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - numpy=1.21.2\n",
    "  - pip=21.2.4\n",
    "  - scikit-learn=0.24.2\n",
    "  - scipy=1.7.1\n",
    "  - pandas>=1.1,<1.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1100f750",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolofromdocker:3\n",
      "catsanddogsenv:1\n",
      "helloworldenv:1\n",
      "AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu:10\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "#give our environment a name\n",
    "custom_env_name = \"helloworldenv\"\n",
    "\n",
    "helloworldenv = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for our hello world example\",\n",
    "    tags={\"scikit-learn\": \"0.24.2\"}, #you can add tags to your environment\n",
    "    conda_file= \"../environments/helloworld.yaml\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\", #you have to supply a base docker image. This one is from AzureML\n",
    "    )\n",
    "\n",
    "# We already have this in the workspace, so this is just to show how to create a new one. No need to run it.\n",
    "# ml_client.environments.create_or_update(helloworldenv)\n",
    "\n",
    "# List all environments in the workspace along with their latest versions. \n",
    "for env in ml_client.environments.list():\n",
    "    print(f\"{env.name}:{env.latest_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ade35",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Environments from docker specifications\n",
    "\n",
    "If you want to use a custom docker specification to build an environment, you can do that. This is useful when your dependencies require very specific OS tweaks. For example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f15044b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment, BuildContext\n",
    "\n",
    "env_docker_context = Environment(\n",
    "    build=BuildContext(path=\"azureml-environment\"), # Path to the directory containing your Dockerfile\n",
    "    name=\"yolofromdocker\",\n",
    "    description=\"Environment created from a Docker context.\",\n",
    ")\n",
    "## Uncomment to create the environment\n",
    "# ml_client.environments.create_or_update(env_docker_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ee0461",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So then we have our environments in the studio:\n",
    "\n",
    "![environments](./images/environments.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b115d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Commands and Jobs\n",
    "We can now bring stuff together to actually do some work!\n",
    "We can define `commands`, and submit those to our compute as `jobs`.\n",
    "A `command` requires information on:\n",
    "- what environment to use\n",
    "- what compute to use\n",
    "- which code context to use\n",
    "- what command to execute.\n",
    "\n",
    "You can think of it as simply spinning up a docker container with a command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "437a14b4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job helloworldjob submitted to the workspace. Monitor it at https://ml.azure.com/runs/helloworldjob?wsid=/subscriptions/cad4de40-836d-424a-bdd6-3296c5c25179/resourcegroups/rgdaniel/workspaces/amldaniel&tid=8b87af7d-8647-4dc7-8df4-5f69a2011bb5\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import command\n",
    "#a job that runs a simple hello world command\n",
    "helloworldjob = command(name=\"helloworldjob\",\n",
    "                        compute=\"defaultcompute\", #supply the compute name we created earlier\n",
    "                        environment=f\"helloworldenv:1\", #specify the environment we created earlier, and the version\n",
    "                        command=\"echo 'Hello World!'\") #the command we want to run, this can be a call to a python script, or a bash script or whatever.\n",
    "\n",
    "job = ml_client.jobs.create_or_update(helloworldjob) #submit the job to the workspace\n",
    "print(f\"Job {job.name} submitted to the workspace. Monitor it at {job.studio_url}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aa2d3d",
   "metadata": {},
   "source": [
    "![helloworldjob](./images/helloworldjob.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4de5a01",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "We need access to data to do things with. We can do this in various ways \n",
    "- register datasets\n",
    "    - these are versioned and directly accesible through AML\n",
    "- connect to a storage account directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51074f9f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catsanddogs_flat:1\n",
      "coco128:2\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "cats_and_dogs_data = Data(\n",
    "                          name=\"catsanddogs_flat\", \n",
    "                          path=\"../data/catsanddogs_flat\",\n",
    "                          type=AssetTypes.URI_FOLDER,\n",
    "                          description=\"A dataset containing images of cats and dogs\"\n",
    "                          )\n",
    "\n",
    "# ml_client.data.create_or_update(cats_and_dogs_data) #submit the data to the workspace\n",
    "\n",
    "datasets = ml_client.data.list() #list all datasets in the workspace\n",
    "for dataset in datasets:\n",
    "    print(f\"{dataset.name}:{dataset.latest_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a4043e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To connect containers in your storage account, you need to find `connections` in the portal and navigate to the container:\n",
    "![container connections](./images/connectstorage.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4880fc5c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azureml_globaldatasets:AzureBlob\n",
      "container_processed:AzureBlob\n",
      "container_raw:AzureBlob\n",
      "workspacefilestore:AzureFile\n",
      "workspaceblobstore:AzureBlob\n",
      "workspaceworkingdirectory:AzureFile\n",
      "workspaceartifactstore:AzureBlob\n"
     ]
    }
   ],
   "source": [
    "#these are available through datastores:\n",
    "\n",
    "datastores = ml_client.datastores.list() #list all datastores in the workspace\n",
    "for datastore in datastores:\n",
    "    print(f\"{datastore.name}:{datastore.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba064c00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pipelines!\n",
    "\n",
    "We have all the pieces we need to start utilizing the power of AzureML pipelines! You can think of a pipeline as a bunch of jobs cobbled together through their `Inputs` and `Outputs`. \n",
    "\n",
    "#### Inputs\n",
    "- `str`, `int`, `float` etc.\n",
    "- `AzureML Datasets`\n",
    "- `uri folders` --> paths to locations in connected blobstorage\n",
    "- `uri files` --> paths to specific files in connected blobstorage\n",
    "- `models` --> from the registered models\n",
    "\n",
    "#### Outputs\n",
    "- `uri folder`\n",
    "- `uri file`\n",
    "- `custom_model`\n",
    "- `mlflow_model`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab24fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pipeline components\n",
    "\n",
    "If we take a simple `command job` like our Hello World example above and we give it `inputs` and `outputs`, we have the beginning of a usable pipeline component. Lets say we want the component to take out name and write a file to somewhere noting that it did so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d639c817",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job helloworldjob submitted to the workspace. Monitor it at https://ml.azure.com/runs/helloworldjob?wsid=/subscriptions/cad4de40-836d-424a-bdd6-3296c5c25179/resourcegroups/rgdaniel/workspaces/amldaniel&tid=8b87af7d-8647-4dc7-8df4-5f69a2011bb5\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "inputs = {'name': 'Daniel'}\n",
    "outputs = {'output_path': Output(type=\"uri_folder\", path=\"azureml://datastores/container_processed/paths/presentation_examples/hello_world\")}\n",
    "\n",
    "job_component = command(\n",
    "    name=\"helloworldjobcomponent4\",\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    compute=\"defaultcompute\",\n",
    "    environment=f\"helloworldenv:1\",\n",
    "    command=\"echo 'Hello ${{inputs.name}}' >> ${{outputs.output_path}}/hello.txt\"\n",
    ")\n",
    "#submit the job to the workspace\n",
    "# job = ml_client.jobs.create_or_update(job_component)\n",
    "print(f\"Job {job.name} submitted to the workspace. Monitor it at {job.studio_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d50c0c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![alt text](./images/helloworkdjobcomponent4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507f763",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./images/helloworldjobcomponentoutputpath.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230e6b2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pipeline components\n",
    "\n",
    "You can tell by the job name that jobs cannot be easily rerun, and specifying the inputs and outputs in code like this all the time is also not optimal. This is where the `pipeline decorator` comes in. We can create a function that packages this job into a pipeline that is rerunnable and deployable. We redefine the job to write the name parameter in the filename, and append to the file with a timestamped line in case it exists. That will illustrate a pipeline component doing its thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd9fa04d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>helloworld_pipeline_presentation</td><td>green_net_fhtn7d3jxf</td><td>pipeline</td><td>NotStarted</td><td><a href=\"https://ml.azure.com/runs/green_net_fhtn7d3jxf?wsid=/subscriptions/cad4de40-836d-424a-bdd6-3296c5c25179/resourcegroups/rgdaniel/workspaces/amldaniel&amp;tid=8b87af7d-8647-4dc7-8df4-5f69a2011bb5\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {'name': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7fa1330d6bc0>}, 'outputs': {'output_path': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7fa1330d6b90>}, 'jobs': {}, 'component': PipelineComponent({'latest_version': None, 'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': None, 'base_path': '/home/daniel/repos/aml_demo/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fa13336a3b0>, 'version': '1', 'schema': None, 'type': 'pipeline', 'display_name': 'helloworld_pipeline', 'is_deterministic': None, 'inputs': {'name': {}}, 'outputs': {'output_path': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'write_name_component_instance': Command({'parameters': {}, 'init': False, 'name': 'write_name_component_instance', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': '/home/daniel/repos/aml_demo/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fa133369b70>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': 'defaultcompute', 'services': None, 'comment': None, 'job_inputs': {'name': '${{parent.inputs.name}}'}, 'job_outputs': {'output_path': '${{parent.outputs.output_path}}'}, 'inputs': {'name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7fa133369ea0>}, 'outputs': {'output_path': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7fa161684970>}, 'component': 'azureml_anonymous:dd06fc1d-1fe1-425f-821f-4c8e95247ac5', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '152a978d-f96f-49b1-a436-64289b81e1e6', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'parent_job_name': None, 'swept': False})}, 'job_types': {'command': 1}, 'job_sources': {'BUILDER': 1}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'NotStarted', 'log_files': None, 'name': 'green_net_fhtn7d3jxf', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': '/subscriptions/cad4de40-836d-424a-bdd6-3296c5c25179/resourceGroups/rgdaniel/providers/Microsoft.MachineLearningServices/workspaces/amldaniel/jobs/green_net_fhtn7d3jxf', 'Resource__source_path': '', 'base_path': '/home/daniel/repos/aml_demo/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7fa13336ad40>, 'serialize': <msrest.serialization.Serializer object at 0x7fa1330d6a40>, 'display_name': 'helloworld_pipeline', 'experiment_name': 'helloworld_pipeline_presentation', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://westeurope.api.azureml.ms/mlflow/v1.0/subscriptions/cad4de40-836d-424a-bdd6-3296c5c25179/resourceGroups/rgdaniel/providers/Microsoft.MachineLearningServices/workspaces/amldaniel?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/green_net_fhtn7d3jxf?wsid=/subscriptions/cad4de40-836d-424a-bdd6-3296c5c25179/resourcegroups/rgdaniel/workspaces/amldaniel&tid=8b87af7d-8647-4dc7-8df4-5f69a2011bb5', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ai.ml import dsl\n",
    "\n",
    "write_name_component = command(\n",
    "    name=\"write_name_component\",\n",
    "    description=\"Write name to file\",\n",
    "    inputs={\"name\": Input(type=\"string\")},\n",
    "    outputs={\"output_path\": Output(type=\"uri_folder\", path=\"azureml://datastores/container_processed/paths/presentation_examples/hello_world_presentation\")},\n",
    "    compute=\"defaultcompute\",\n",
    "    environment=f\"helloworldenv:1\",\n",
    "    command=\"echo $(date '+%Y-%m-%d %H:%M:%S') Hello ${{inputs.name}} >> ${{outputs.output_path}}/hello_${{inputs.name}}.txt\"\n",
    ")\n",
    "@dsl.pipeline(experiment_name=\"helloworld_pipeline_presentation\")\n",
    "def helloworld_pipeline(name: Input):\n",
    "    write_name_component_instance = write_name_component(name=name)\n",
    "    return {\"output_path\": write_name_component_instance.outputs.output_path}\n",
    "\n",
    "pipeline_job = helloworld_pipeline(name=\"Analytics_Bergen!\") #create the pipeline job\n",
    "ml_client.jobs.create_or_update(pipeline_job) #submit the pipeline job to the workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2227423",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./images/write_name_component_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b108e14a",
   "metadata": {},
   "source": [
    "![](./images/write_name_component_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77c8f4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Chaining components\n",
    "Now we have shown one component with inputs and outputs, but we can chain them together and 'orchestrate powerful pipelines' as they would have us say. Lets add a simple component that just reads the file we wrote back to us. We will redefine the write_name_component for this pipeline since output paths are experiment specific, meaning that a new experiment will fail if you run it towards a non-empty directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fec51fb1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>echocat_pipeline_presentation</td><td>cool_parcel_kr5nydl1qq</td><td>pipeline</td><td>NotStarted</td><td><a href=\"https://ml.azure.com/runs/cool_parcel_kr5nydl1qq?wsid=/subscriptions/cad4de40-836d-424a-bdd6-3296c5c25179/resourcegroups/rgdaniel/workspaces/amldaniel&amp;tid=8b87af7d-8647-4dc7-8df4-5f69a2011bb5\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {'name': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7fa133337880>}, 'outputs': {}, 'jobs': {}, 'component': PipelineComponent({'latest_version': None, 'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': None, 'base_path': '/home/daniel/repos/aml_demo/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fa133337310>, 'version': '1', 'schema': None, 'type': 'pipeline', 'display_name': 'echocat_pipeline', 'is_deterministic': None, 'inputs': {'name': {}}, 'outputs': {}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'write_name_component_instance': Command({'parameters': {}, 'init': False, 'name': 'write_name_component_instance', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': '/home/daniel/repos/aml_demo/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fa133080b80>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': 'defaultcompute', 'services': None, 'comment': None, 'job_inputs': {'name': '${{parent.inputs.name}}'}, 'job_outputs': {'output_path': {'type': 'uri_folder', 'mode': 'rw_mount'}}, 'inputs': {'name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7fa133080a30>}, 'outputs': {'output_path': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7fa133080730>}, 'component': 'azureml_anonymous:d9001201-6d3b-4058-b9cd-0a8e5fef46d5', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '74ba2b40-5039-40d3-a172-bc7aa836b349', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'parent_job_name': None, 'swept': False}), 'cat_name_component_instance': Command({'parameters': {}, 'init': False, 'name': 'cat_name_component_instance', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': None, 'Resource__source_path': '', 'base_path': '/home/daniel/repos/aml_demo/notebooks', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fa133336b60>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': 'defaultcompute', 'services': None, 'comment': None, 'job_inputs': {'filepath': '${{parent.jobs.write_name_component_instance.outputs.output_path}}', 'name': '${{parent.inputs.name}}'}, 'job_outputs': {}, 'inputs': {'filepath': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7fa133336e90>, 'name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7fa133337940>}, 'outputs': {}, 'component': 'azureml_anonymous:2fdfed93-fab7-4758-bbec-2035ed792761', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': 'dd0d8f12-2832-415e-9f14-5f4354cdc8b2', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'parent_job_name': None, 'swept': False})}, 'job_types': {'command': 2}, 'job_sources': {'BUILDER': 2}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'NotStarted', 'log_files': None, 'name': 'cool_parcel_kr5nydl1qq', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': '/subscriptions/cad4de40-836d-424a-bdd6-3296c5c25179/resourceGroups/rgdaniel/providers/Microsoft.MachineLearningServices/workspaces/amldaniel/jobs/cool_parcel_kr5nydl1qq', 'Resource__source_path': '', 'base_path': '/home/daniel/repos/aml_demo/notebooks', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7fa1333375e0>, 'serialize': <msrest.serialization.Serializer object at 0x7fa133337820>, 'display_name': 'echocat_pipeline', 'experiment_name': 'echocat_pipeline_presentation', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://westeurope.api.azureml.ms/mlflow/v1.0/subscriptions/cad4de40-836d-424a-bdd6-3296c5c25179/resourceGroups/rgdaniel/providers/Microsoft.MachineLearningServices/workspaces/amldaniel?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/cool_parcel_kr5nydl1qq?wsid=/subscriptions/cad4de40-836d-424a-bdd6-3296c5c25179/resourcegroups/rgdaniel/workspaces/amldaniel&tid=8b87af7d-8647-4dc7-8df4-5f69a2011bb5', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_name_component = command(\n",
    "    name=\"write_name_component\",\n",
    "    description=\"Write name to file\",\n",
    "    inputs={\"name\": Input(type=\"string\")},\n",
    "    outputs={\"output_path\": Output(type=\"uri_folder\", mode = \"rw_mount\")},\n",
    "    compute=\"defaultcompute\",\n",
    "    environment=f\"helloworldenv:1\",\n",
    "    command=\"echo $(date '+%Y-%m-%d %H:%M:%S') Hello ${{inputs.name}} >> ${{outputs.output_path}}/hello_${{inputs.name}}.txt\"\n",
    ")\n",
    "\n",
    "cat_name_component = command(\n",
    "    name=\"cat_name_component\",\n",
    "    description=\"write the contents of the name file to the console\",\n",
    "    inputs={\"filepath\" : Input(type=\"uri_folder\"), \"name\": Input(type=\"string\")},\n",
    "    compute=\"defaultcompute\",\n",
    "    environment=\"helloworldenv:1\",\n",
    "    command=\"cat ${{inputs.filepath}}/hello_${{inputs.name}}.txt\"\n",
    ")\n",
    "\n",
    "@dsl.pipeline(experiment_name=\"echocat_pipeline_presentation\")\n",
    "def echocat_pipeline(name: Input): \n",
    "    write_name_component_instance = write_name_component(name=name)\n",
    "    cat_name_component_instance = cat_name_component(filepath=write_name_component_instance.outputs.output_path, name=name)\n",
    "\n",
    "pipeline_job = echocat_pipeline(name=\"analytics_bergen!\") #create the pipeline job\n",
    "ml_client.jobs.create_or_update(pipeline_job) #submit the pipeline job to the workspace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace434f",
   "metadata": {},
   "source": [
    "![](./images/echocat_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25238299",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pipelines can be published (legacy?) or deployed in the portal. This makes them reusable from endpoints and form the ui in the studio so others can run the pipelines on new data and collect results. That is how you can make your processing and analysis available to others in a scalable way. \n",
    "\n",
    "For more complex examples with better real-world applicability, you can have a look at the various notebooks in this repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23c507a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "thanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "339f3cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: detectandsegment2, ID: /subscriptions/cad4de40-836d-424a-bdd6-3296c5c25179/resourceGroups/rgdaniel/providers/Microsoft.MachineLearningServices/workspaces/amldaniel/batchEndpoints/detectandsegment2\n"
     ]
    }
   ],
   "source": [
    "for endpoint in ml_client.batch_endpoints.list():\n",
    "    print(f\"Endpoint name: {endpoint.name}, ID: {endpoint.id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89887740",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_job = ml_client.batch_endpoints.invoke(endpoint_name=\"detectandsegment2\", \n",
    "                                                inputs={\"name\": Input(type=\"string\", default=\"Daniel_from_endpoint_invocation!\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a544d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd8958e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7b4973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "endpoint_job = ml_client.batch_endpoints.invoke(\n",
    "    endpoint_name=\"echocat\",\n",
    "    inputs={\"name\": Input(type=\"string\", default=\"Daniel_from_endpoint_invocation_again!\")},\n",
    "    job_name=f\"echocat_job_{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "    description=\"Job created from endpoint invocation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bbc493f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a unique path with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_path = f\"azureml://datastores/container_processed/paths/presentation_examples_{timestamp}/hello_world_presentation_{timestamp}\"\n",
    "endpoint_job2 = ml_client.batch_endpoints.invoke(\n",
    "    endpoint_name=\"echocat\",\n",
    "    experiment_name=\"helloworld_pipeline_presentation_from_endpoint\",\n",
    "    job_name=\"helloworld_pipeline_presentation_from_endpoint_job\",\n",
    "    inputs={\"name\": Input(type=\"string\", default=\"Daniel_from_endpoint_invocation_again\")},\n",
    "    outputs={\"output_path\": Output(type=\"uri_folder\", path=output_path) } # Use the unique path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7913c0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
